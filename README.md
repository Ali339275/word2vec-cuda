# CUDA-Based Skip-Gram with Negative Sampling (SGNS)

This project implements the Skip-Gram with Negative Sampling (SGNS) algorithm in:

- CUDA (C++ GPU implementation)
- Python (PyTorch CPU baseline)

The goal is to compare performance and convergence between both implementations.

---

# 1ï¸âƒ£ Project Structure

```
w2v_cpp_project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cpu/                # PyTorch implementation
â”‚   â”œâ”€â”€ w2v_base_cuda/      # CUDA kernels (baseline)
â”‚   â”œâ”€â”€ build/              # Compiled CUDA binaries
â”‚
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ README.md
```

---

# 2ï¸âƒ£ Build Instructions

## ğŸ”¹ Build CUDA Version

From the project root:

```bash
make clean
cmake ..
make -j
```

Or manually using CMake:

```bash
mkdir -p src/build
cd src/build
cmake ..
make -j
```

---

## ğŸ”¹ Build Python Extension (if applicable)

```bash
python setup.py build_ext --inplace
```

---

# 3ï¸âƒ£ Running the Implementations

## ğŸš€ CUDA Version

From the build directory:

```bash
cd ./src/build

srun --gres=gpu:1 --cpus-per-task=4 --mem=2GB ./w2v_base_cuda_train \
  --emb-dim 128 \
  --batch-size 512 \
  --epochs 15
```

### Parameters

- `--emb-dim` â†’ Embedding dimension
- `--batch-size` â†’ Batch size
- `--epochs` â†’ Number of training epochs

---

## ğŸ PyTorch Version

```bash
cd ./src/cpu

srun --gres=gpu:1 --cpus-per-task=4 --mem=2GB \
python ./main.py \
  --embedding_dim 128 \
  --batch_size 512 \
  --epochs 15
```

### Parameters

- `--embedding_dim` â†’ Embedding dimension
- `--batch_size` â†’ Batch size
- `--epochs` â†’ Number of epochs

---

# 4ï¸âƒ£ Dataset

The project uses:

```
data/text8_500k
```

A 500k-token subset of the text8 corpus.

---

# 5ï¸âƒ£ Output

After training:

- CUDA version outputs:

```
word_embeddings_cuda_base_stable.bin
```

- PyTorch version outputs:

```
word_embeddings.pt
```

---

# 6ï¸âƒ£ Performance Comparison

The project compares:

- Training time
- Speedup
- Loss convergence

Speedup is computed as:

```
Speedup = CPU Time / GPU Time
```

---

# 7ï¸âƒ£ Requirements

- CUDA Toolkit
- CMake
- GCC (with CUDA support)
- Python 3.x
- PyTorch
- SLURM (for `srun` execution)

---

# ğŸ” Reproducibility Settings

To reproduce the reported results:

- Batch size = 512
- Learning rate = 0.01
- Epochs = 15
- Negative samples = 60
- Window size = 1

---

This project demonstrates how GPU parallelism using CUDA can significantly accelerate computationally intensive natural language processing tasks such as training word embeddings with SGNS.
