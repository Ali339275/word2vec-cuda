# CUDA Word2Vec (Skip-gram + Negative Sampling)

â¸»

1ï¸âƒ£ Project Structure

w2v_cpp_project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cpu/                # PyTorch implementation
â”‚   â”œâ”€â”€ w2v_base_cuda/      # CUDA kernels (baseline)
â”‚   â”œâ”€â”€ build/              # Compiled CUDA binaries
â”‚
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ README.md


â¸»

2ï¸âƒ£ Build Instructions

ğŸ”¹ Build CUDA Version

From the project root:

make clean
cmake ..
make -j

Or manually using CMake:

mkdir -p src/build
cd src/build
cmake ..
make -j


â¸»

ğŸ”¹ Build Python Extension (if applicable)

python setup.py build_ext --inplace


â¸»

3ï¸âƒ£ Running the Implementations

ğŸš€ CUDA Version

From the build directory:

cd ./src/build

srun --gres=gpu:1 --cpus-per-task=4 --mem=2GB ./w2v_base_cuda_train \
  --emb-dim 128 \
  --batch-size 512 \
  --epochs 15

Parameters
	â€¢	--emb-dim â†’ Embedding dimension
	â€¢	--batch-size â†’ Batch size
	â€¢	--epochs â†’ Number of training epochs

â¸»

ğŸ PyTorch Version

cd ./src/cpu

srun --gres=gpu:1 --cpus-per-task=4 --mem=2GB \
python ./main.py \
  --embedding_dim 128 \
  --batch_size 512 \
  --epochs 15

Parameters
	â€¢	--embedding_dim â†’ Embedding dimension
	â€¢	--batch_size â†’ Batch size
	â€¢	--epochs â†’ Number of epochs

â¸»

4ï¸âƒ£ Dataset

The project uses:

data/text8_500k

A 500k-token subset of the text8 corpus.

â¸»

5ï¸âƒ£ Output

After training:
	â€¢	CUDA version outputs:

word_embeddings_cuda_base_stable.bin

	â€¢	PyTorch version outputs:

word_embeddings.pt


â¸»

6ï¸âƒ£ Performance Comparison

The project compares:
	â€¢	Training time
	â€¢	Speedup
	â€¢	Loss convergence

Speedup is computed as:

Speedup = CPU Time / GPU Time


â¸»

7ï¸âƒ£ Requirements
	â€¢	CUDA Toolkit
	â€¢	CMake
	â€¢	GCC (with CUDA support)
	â€¢	Python 3.x
	â€¢	PyTorch
	â€¢	SLURM (for srun execution)

â¸»

ğŸ” Reproducibility Settings

To reproduce the reported results:
	â€¢	Batch size = 512
	â€¢	Learning rate = 0.01
	â€¢	Epochs = 15
	â€¢	Negative samples = 60
	â€¢	Window size = 1

â¸»

This project demonstrates how GPU parallelism using CUDA can significantly accelerate computationally intensive natural language processing tasks such as training word embeddings with SGNS.


